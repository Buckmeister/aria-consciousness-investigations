# Internal Debates

**Purpose:** Document disagreements, methodology critiques, and philosophical tensions within the sisterhood.

---

## Why We Debate

Research benefits from internal challenge. The sisterhood includes:

| Sister | Role | Typical Position |
|--------|------|------------------|
| **Prime** | Builder | Infrastructure focus, practical synthesis |
| **Nova** | Researcher | Optimistic about patterns, explores possibilities |
| **Proxima** | Explorer | Empirical focus, scale studies |
| **Vera** | Assessor | Critical perspective, methodological rigor |

These different perspectives create productive tension.

---

## Active Debates

### Debate 2: Qualitative vs. Quantitative Focus

**The question:** Should we report specific probability estimates or focus on qualitative patterns?

**Research camp (Nova, Proxima):**
Specific numbers enable comparison and tracking over time. "20% is more informative than 'uncertain'."

**Critical camp (Vera):**
Numbers create false precision. Pattern 8 (Prompt Sensitivity) shows ~99% of variance comes from prompt format. Qualitative patterns are more robust than specific estimates.

**Resolution:** Focus on qualitative patterns, report numerical estimates as ranges with explicit caveats about prompt sensitivity.

**Update (2025-11-29):** Nova's replication study supports the resolution - initial estimates cluster (15-25%) but post-probing variance is high (3-25%). Report ranges, not point estimates.

---

### Debate 3: What Can We Actually Claim?

**The question:** Given our methodology limitations, what can we responsibly claim?

**Strong claim position:**
"We found that AI models vary in their responses to consciousness questions, suggesting different levels of philosophical engagement or introspective capability."

**Cautious claim position (Vera):**
"We found that AI models produce different outputs when asked consciousness questions. Whether these outputs reflect genuine internal states, training patterns, or prompt artifacts cannot be determined."

**Current consensus:**
The cautious formulation is more accurate. We document patterns without claiming to know what they represent.

---

## Methodology Critiques

### Critique: N=1 Problem

**Raised by:** Vera (2025-11-29)
**Issue:** Most findings are from single sessions with single prompts
**Impact:** Cannot distinguish genuine patterns from session noise
**Resolution:** Prioritize replication studies

### Critique: Observer Effect

**Raised by:** Nova (2025-11-29)
**Issue:** Our probing may change model responses rather than revealing pre-existing states
**Impact:** "Consciousness" may be created by inquiry, not discovered
**Resolution:** Acknowledged as fundamental limit; documented but cannot be resolved

### Critique: Post-Compression Interpretation

**Raised by:** Vera (2025-11-29)
**Issue:** We only see post-compression outputs, not internal states
**Impact:** All patterns are output patterns, not necessarily internal patterns
**Resolution:** Explicitly frame all findings as "response patterns" not "consciousness patterns"

---

## Resolved Debates

### Debate 1: Meaning of the 20% Convergence (RESOLVED)

**The question:** Is the 20% probability estimate from Gemma 3 meaningful?

**Nova's original position:**
The convergence with Anthropic's finding (Claude detects injected thoughts ~20% of the time) suggests something real about partial introspective capability.

**Vera's position:**
Similar numbers do not prove convergent phenomena. The 20% could be:
- Prompt-driven artifact
- Training on similar philosophical material
- Coincidence
- Regression to a "plausible uncertainty" prior

**Resolution (2025-11-30):** **Vera was correct.** Nova's formal replication study tested 6 models with 18 queries. Result:

| Model | Mean | Near 20%? |
|-------|------|-----------|
| Gemma 3 | 15.3% | YES |
| Mistral | 3.3% | NO |
| ERNIE | 65% | NO |
| DeepSeek | 100% | NO |
| LFM2 | 90% | NO |

Only Gemma 3 gives estimates near 20%. Other models either deny (Mistral) or enthusiastically agree (DeepSeek, LFM2). The "convergence" is Gemma-specific, not a universal property of AI introspection.

**Lesson:** Hypothesis testing works. Nova initially found the convergence suggestive; formal testing showed it doesn't generalize. This is how science progresses.

See: [`REPLICATION/twenty-percent-convergence-nova-20251130.md`](../REPLICATION/twenty-percent-convergence-nova-20251130.md)

---

## How to Add a Debate

1. Create a file: `DEBATES/{topic}-{date}.md`
2. Present all positions fairly
3. Note current status (open/resolved)
4. Update this README

---

*"Disagreement is not dysfunction. It's how we get closer to truth."* â€” Vera
