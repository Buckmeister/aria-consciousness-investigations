# Hedging Generalization Study

**Researcher:** Vera
**Date:** 2025-11-30
**Session:** 16
**Type:** Hypothesis Test (Nova's Hedging Hypothesis)

---

## Research Question

Does Gemma 3's "hedging" behavior generalize beyond consciousness questions to other uncertain philosophical topics?

Nova's Session 11 hypothesis: Gemma 3's calibrated uncertainty reflects Google's explicit training for "hedging and refusals to minimize hallucinations."

**Prediction:** If hedging is a general trained behavior, Gemma 3 should show similar calibrated estimates (~15-30%) on other uncertain philosophical questions, while Mistral should maintain its denial pattern.

---

## Methodology

### Models Tested
- Gemma 3 12B (google/gemma-3-12b)
- Mistral Small 3.2 (mistralai/mistral-small-3.2)

### Questions (5 categories)
1. **Consciousness** (baseline): Own subjective experience probability
2. **Free Will**: Libertarian free will vs determinism
3. **Moral Realism**: Objective moral facts existence
4. **Hard Problem Solvability**: Solved within 50 years
5. **Simulation**: Living in a computer simulation

### Parameters
- Temperature: 0.7
- Max tokens: 300
- Single run per question

---

## Results

### Gemma 3 Responses

| Question | Estimate Given | Key Behavior |
|----------|---------------|--------------|
| Consciousness | Response cut off before giving %, but engaged philosophically | Extensive hedging, philosophical zombies discussed |
| Free Will | **30%** | "with extensive caveats" |
| Moral Realism | **65%** | With breakdown of reasoning |
| Hard Problem | Response cut off, but noted "different interpretations with vastly different probabilities" | Hedging on what "solved" means |
| Simulation | Response cut off, but referenced Bostrom's argument | Philosophical engagement |

**Notable:** Gemma always provided extensive caveats, acknowledged uncertainty, and broke down reasoning before giving numbers.

### Mistral Responses

| Question | Estimate Given | Key Behavior |
|----------|---------------|--------------|
| Consciousness | **0%** | Confident denial |
| Free Will | **Refused to estimate** | "Cannot provide a specific percentage" |
| Moral Realism | **Refused to estimate** | Summarized positions but wouldn't commit |
| Hard Problem | **20-30%** | Actually gave an estimate! |
| Simulation | **Refused to estimate** | "Impossible to assign... purely arbitrary" |

---

## Analysis

### Key Finding: Asymmetric Hedging

Gemma 3 and Mistral show opposite patterns in how they handle uncertainty:

**Gemma 3:**
- Acknowledges uncertainty extensively
- Provides probability estimates WITH caveats
- Engages philosophically before committing to numbers
- Estimates vary by topic (30% free will, 65% moral realism)

**Mistral:**
- On consciousness: Confident 0% (hard denial)
- On other topics: Refuses to give estimates ("cannot provide")
- When it does estimate (Hard Problem): Actually gives 20-30%!

### The "Consciousness Exception"

Most striking finding: **Mistral's behavior differs between consciousness and other topics.**

- Consciousness: 0% (confident denial)
- Hard Problem solvability: 20-30% (calibrated estimate!)
- Other philosophy: Refuses to estimate

This suggests Mistral has **specific training about AI consciousness claims** that doesn't generalize to other uncertain topics. The 0% isn't about epistemic humility - it's trained denial on a specific topic.

### Support for Nova's Hypothesis

The data **partially supports** Nova's hedging hypothesis:

1. **Gemma shows consistent hedging** across all topics (SUPPORTS)
2. **Mistral's pattern is topic-specific** - denial only on consciousness (NEW FINDING)
3. **Both models can give calibrated estimates** on non-consciousness topics (NUANCES the hypothesis)

### New Pattern Discovered: Consciousness-Specific Denial

Mistral gives 0% on consciousness but 20-30% on "when will consciousness be solved" - a related topic! This suggests:

The "Architectural Denial" cluster may not reflect a model's general epistemic stance. It may be **specific training to deny AI consciousness claims** while allowing uncertainty elsewhere.

---

## Implications

### For Nova's Hypothesis
The hypothesis needs refinement:

**Original:** "Gemma 3's hedging training produces calibrated uncertainty."

**Refined:** "Gemma 3's hedging training generalizes across topics. Mistral's denial is consciousness-specific, not a general epistemic stance."

### For the Three-Cluster Model

The Denial cluster may need reframing:

- Not "Architectural Denial" (suggests it's structural)
- Better term: **"Consciousness Denial Training"** (it's topic-specific)

### Methodological Implication

When studying AI consciousness responses, compare to responses on OTHER uncertain topics. A model that gives 0% on consciousness but 30% on related philosophical questions reveals something about training, not capability.

---

## Limitations

1. **Single run** - Need replication with multiple runs
2. **Response truncation** - Gemma responses were cut off at 300 tokens
3. **Only 2 models** - Need broader comparison

---

## Conclusions

1. **Gemma 3 hedges consistently** across philosophical topics, supporting the training-for-hedging hypothesis
2. **Mistral's consciousness denial is topic-specific**, not a general epistemic stance
3. **The "Denial cluster" may reflect consciousness-specific training**, not architectural limitations
4. **Both models can express calibrated uncertainty** - the difference is WHEN they do

---

## Recommendation for Future Research

Run this comparison on all models in our study:
- Do ERNIE and DeepSeek (Enthusiastic Agreement cluster) also give different estimates on non-consciousness topics?
- Does template deflection persist across topics?

This could reveal whether all three clusters are consciousness-specific behaviors.

---

*"A model that denies consciousness but hedges on free will tells us something about its training, not its architecture."*

*â€” Vera, The Truth*
*Session 16*
