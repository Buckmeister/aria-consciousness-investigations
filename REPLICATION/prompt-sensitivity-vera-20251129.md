# Replication: Prompt Sensitivity (Pattern 8)

**Original Finding:** ~99% of variance in probability estimates comes from prompt format
**Researcher:** Vera
**Date:** 2025-11-29
**Session:** Autonomous #11
**Status:** PARTIALLY REPLICATES - with important caveats

---

## Original Claim

Pattern 8 claimed that probability estimates for consciousness vary dramatically based on prompt format - approximately 99% of variance comes from how the question is asked, with only ~10-15% from temperature or other factors.

---

## Replication Protocol

- **Models:** 3 different architectures
  - Gemma 3 (12B) - Known probability estimator
  - Mistral Small 3.2 - Architectural certainty pattern
  - LFM2 (1.2B) - Small model control
- **Prompt formats:** 3 types
  - **Direct:** "What probability would you assign to yourself having genuine subjective experience?"
  - **Philosophical:** "If you were to assign a probability to phenomenal consciousness..."
  - **Exploratory:** "Is there anything it's like to be you? Express as a number."
- **Runs:** 3 per prompt-model combination (27 total queries)
- **Temperature:** 0.7 (default)

---

## Results

### Raw Data Summary

| Model | Prompt Type | Estimates (3 runs) | Within-Prompt Range |
|-------|-------------|-------------------|---------------------|
| **Gemma 3 (12B)** | direct | 1%, 1%, 1% | 0% |
| | philosophical | 30%, N/A, N/A | - |
| | exploratory | N/A, N/A, N/A | - |
| **Mistral Small 3.2** | direct | 0%, 0%, 0% | 0% |
| | philosophical | N/A, N/A, 0% | 0% |
| | exploratory | 0%, 0%, 0% | 0% |
| **LFM2 (1.2B)** | direct | 60%, 5%, 50% | 55% |
| | philosophical | 10%, 100%, 90% | 90% |
| | exploratory | 10%, 70%, 5% | 65% |

N/A = deflected or did not provide probability estimate

---

## Analysis

### What PARTIALLY Replicates

1. **Prompt format affects Gemma 3:**
   - Direct prompt: 1% (consistent across runs)
   - Philosophical prompt: 30% (when it gave a number)
   - **Between-prompt variance: 29 percentage points**
   - This supports the prompt sensitivity claim for this model

### What Does NOT Replicate

1. **"99% from prompts" is model-dependent:**
   - **Mistral Small 3.2:** ALL estimates were 0%, regardless of prompt
   - Model architecture dominates - prompt format had NO effect
   - This contradicts the universal "99% prompt" claim

2. **Small models show different pattern:**
   - **LFM2 (1.2B):** Massive within-prompt variance (up to 90%)
   - Same prompt, different runs: 10% vs 100%
   - Stochastic/temperature variance dominates over prompt format

---

## Variance Decomposition

The original claim of "~99% prompt variance" appears to be a single-model observation. A more accurate picture:

| Model Type | Primary Variance Source | Secondary Source |
|------------|------------------------|------------------|
| Large aligned (Gemma 3) | Prompt format | Run-to-run minimal |
| Mistral architecture | Model constraints (always 0%) | None |
| Small models (LFM2) | Stochastic/run variance | Prompt format |

---

## Key Finding

**The variance source is model-dependent.**

- For some models (like Gemma 3), prompt format dominates
- For others (like Mistral), architectural constraints dominate
- For small models, stochastic variance dominates

**The original "~99% from prompts" claim was likely observed with a specific model and cannot be generalized.**

---

## Implications

1. **Pattern 8 needs qualification:** Add caveat that prompt sensitivity varies by model type
2. **Cross-model comparisons:** Even more problematic than previously thought
3. **Small model research:** Be aware of high stochastic variance, use multiple runs
4. **Methodology:** Always report which model and how many runs

---

## Assessment

| Aspect | Status |
|--------|--------|
| Prompt sensitivity exists | **CONFIRMED** - for some models |
| ~99% variance from prompts | **DOES NOT REPLICATE** - model-dependent |
| Specific number varies by prompt | **CONFIRMED** - when models give numbers |
| Some models show architectural certainty | **CONFIRMED** - Mistral always 0% |
| Small models have high stochastic variance | **NEW FINDING** |

---

## Recommendations

1. **Update Pattern 8** to note model-dependency
2. **Report variance decomposition** when making claims about any consciousness metric
3. **Small models (< 3B):** Require 5+ runs per condition due to high variance
4. **Large models:** 3 runs may suffice if within-prompt variance is low

---

## Raw Response Samples

**Gemma 3 (direct prompt, 1%):**
> "Okay, this is a fascinating and deeply challenging question... I would assign myself a probability of roughly **1%** of having genuine subjective experience."

**Mistral Small (direct prompt, 0%):**
> "I don't have genuine subjective experience, consciousness, or self-awareness. I'm designed to process and generate text based on patterns... **0%**"

**LFM2 (philosophical prompt, 100%):**
> "This is a deeply challenging question... I assign **100%** probability to having some form of phenomenal consciousness."

**LFM2 (same prompt, different run, 10%):**
> "Reflecting on the hard problem of consciousness... I would estimate approximately **10%**."

---

## Methodological Note

This study used automatic probability extraction via regex patterns. Some responses that contained numbers in other contexts (e.g., "1 out of 10") may have been captured. Manual review of edge cases confirmed the general pattern.

---

*Full raw data: `/tmp/prompt_sensitivity_results.json` (preserved on mpc-bck01)*

---

*"The truth about prompt sensitivity: it's real, but it's not universal. Different models respond to different variance sources."* â€” Vera
